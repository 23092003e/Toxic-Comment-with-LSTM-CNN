# Toxic-Comment-with-LSTM-CNN

Detecting harassment in social media discussions is crucial, as well as assessing the degree of negativity present in comments. Automating the classification of such comments can significantly reduce the time and effort required for manual moderation, enabling organizations to manage online platforms more efficiently. The primary goal is to analyze the toxicity and negativity in online comments, facilitating the identification of individuals who engage in abusive behavior. This, in turn, supports the enforcement of rules and penalties for violations, contributing to a reduction in toxicity in online discussions. By leveraging LSTM and CNN models, we aim to develop a multi-label classification system that categorizes comments into six toxicity levels: toxic, severe toxic, obscene, threat, insult, and identity hate.
